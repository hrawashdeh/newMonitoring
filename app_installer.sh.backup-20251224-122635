
#!/bin/bash

# ===================== Colors =====================
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
GREEN='\033[0;32m'
RED='\033[0;31m'
NC='\033[0m'   # No Color
SCRIPT_START_EPOCH="$(date +%s)"
SCRIPT_START_STR="$(date +"%Y-%m-%d %H:%M:%S")"

# ===================== Log Helpers =====================
log_info()     { printf "%b[INFO]%b %s\n"    "$BLUE"   "$NC" "$*"; }
log_debug()    { printf "%b[DEBUG]%b %s\n"   "$YELLOW" "$NC" "$*"; }
log_warn()     { printf "%b[WARN]%b %s\n"    "$YELLOW" "$NC" "$*"; }
log_error()    { printf "%b[ERROR]%b %s\n"   "$RED"    "$NC" "$*"; }
log_success()  { printf "%b[SUCCESS]%b %s\n" "$GREEN"  "$NC" "$*"; }
log_section()  { printf "\n\033[0;35m--- %s ---\033[0m\n\n" "$*"; }
log_purple()   { printf "\033[0;35m%s\033[0m\n" "$*"; }
exit_error()   { log_error "$*"; exit 1; }

# ===================== Prompt Helper =====================
prompt_choice() {
  local message="$1"
  local options_str="$2"
  echo message
  echo $options_str
  IFS='/' read -ra OPTIONS <<< "$options_str"

  local DEFAULT="${OPTIONS[0]}"

  while true; do
    printf "${GREEN}%s (%s): ${NC}" "$message" "$options_str"
    read -r CONFIRM < /dev/tty
    CONFIRM=${CONFIRM:-$DEFAULT}

    for opt in "${OPTIONS[@]}"; do

      if [[ "$CONFIRM" == "$opt" ]]; then
        return 0
      fi
    done

    log_error "Invalid option. Allowed values: ${options_str}"
  done
}


# ===================== Configuration =====================
log_section "Verify Configuration"
PROJECT_ROOT="/Volumes/Files/Projects/newLoader"
SEALED_NS="sealed-secrets"
A_NAMESPACE="monitoring-app"
k_context="docker-desktop"

log_info "Switching to project root"
log_info "Please verify the parameters below before proceeding"

log_debug "PROJECT_ROOT = ${PROJECT_ROOT}"
log_debug "SEALED_NS    = ${SEALED_NS}"
log_debug "NAMESPACE    = ${A_NAMESPACE}"
log_debug "k_context    = ${k_context}"

prompt_choice "Is this valid configuration?" "Y/n"
[[ "$CONFIRM" =~ ^[Yy]$ ]] || exit_error "Aborted by user"

# ===================== kubectl Context =====================
log_section "Switch kubectl context"
if ! kubectl config use-context "${k_context}"; then
    exit_error "Failed to switch kubectl context to '${k_context}'"
fi

log_info "Current kubectl context: $(kubectl config current-context)"

prompt_choice "Is this valid context?" "Y/n"
[[ "$CONFIRM" =~ ^[Yy]$ ]] || exit_error "Aborted by user"

# ===================== Namespaces =====================
log_section "Creating namespaces"

kubectl delete namespace monitoring-app --ignore-not-found

kubectl create namespace "${A_NAMESPACE}" --dry-run=client -o yaml | kubectl apply -f -

log_info "Current Kubernetes namespaces:"
kubectl get namespaces

if ! kubectl get namespace "${A_NAMESPACE}" >/dev/null 2>&1; then
   exit_error "Namespace '${A_NAMESPACE}' was not created successfully"
fi


log_info "Namespaces validated successfully"


# ===================== Selaed secret  =====================
cd "${PROJECT_ROOT}/services/secrets" || exit_error "Missing directory: ${PROJECT_ROOT}/services/secrets"

log_section "installing Sealed Secrets "

kubeseal \
  --controller-name=sealed-secrets \
  --controller-namespace=${SEALED_NS} \
  --namespace ${A_NAMESPACE} \
  --format yaml \
  < app-secrets-plain.yaml \
  > app-secrets-sealed.yaml || exit_error "kubeseal failed"

kubectl apply -f app-secrets-sealed.yaml || exit_error "Applying sealed secret failed"



# ===================== Comomon functions ==================

validate_deployment() {
    local deployment=$1
    local namespace=$2
    if ! kubectl get deployment "$deployment" -n "$namespace" &>/dev/null; then
        log_error "Deployment '$deployment' not found in namespace '$namespace'"
        return 1
    fi
    return 0
}


monitor_pod_health() {
    local deployment=$1
    local namespace=$2
    local timeout=${3:-300}
    local elapsed=0
    local interval=10

    log_info "Monitoring pod health for deployment: $deployment"
    log_info "Timeout: ${timeout}s | Check interval: ${interval}s"
    echo

    while [ $elapsed -lt $timeout ]; do
        local ready_replicas=$(kubectl get deployment "$deployment" -n "$namespace" \
 		 -o jsonpath='{.status.readyReplicas}' 2>/dev/null)

		local desired_replicas=$(kubectl get deployment "$deployment" -n "$namespace" \
  		-o jsonpath='{.spec.replicas}' 2>/dev/null)

		ready_replicas=${ready_replicas:-0}
		desired_replicas=${desired_replicas:-0}

        echo -ne "\r${BLUE}[$(date +%H:%M:%S)]${NC} Ready: $ready_replicas/$desired_replicas pods"

        if [ "$ready_replicas" -eq "$desired_replicas" ] && [ "$ready_replicas" != 0 ]; then
            echo
            log_success "All pods are ready ($ready_replicas/$desired_replicas)"
            return 0
        fi

        sleep $interval
        elapsed=$((elapsed + interval))
    done

    echo
    log_error "Timeout: Pods did not become ready within ${timeout}s"

    # Show pod status for debugging
    echo
    log_error "Current pod status:"
    kubectl get pods -n "$namespace" -l "app=$deployment" --no-headers

    return 1
}

get_pod_status() {
    local deployment=$1
    local namespace=$2

    if ! validate_deployment "$deployment" "$namespace"; then
        echo "NOT_FOUND"
        return 1
    fi

    local ready_replicas=$(kubectl get deployment "$deployment" -n "$namespace" \
        -o jsonpath='{.status.readyReplicas}' 2>/dev/null || echo "0")
    local desired_replicas=$(kubectl get deployment "$deployment" -n "$namespace" \
        -o jsonpath='{.spec.replicas}' 2>/dev/null || echo "0")

    if [ "$ready_replicas" -eq "$desired_replicas" ] && [ "$ready_replicas" != "0" ]; then
        echo "HEALTHY"
    elif [ "$ready_replicas" -eq "0" ]; then
        echo "UNAVAILABLE"
    else
        echo "DEGRADED"
    fi
}

scan_pod_logs() {
    local deployment=$1
    local namespace=$2
    local tail_lines=${3:-100}

    log_info "Scanning logs for deployment: $deployment"

    # Get all pods for this deployment
    local pods=$(kubectl get pods -n "$namespace" -l "app=$deployment" \
        -o jsonpath='{.items[*].metadata.name}' 2>/dev/null)

    if [ -z "$pods" ]; then
        log_error "No pods found for deployment: $deployment"
        return 0
    fi

    local error_count=0
    local warn_count=0

    for pod in $pods; do
        echo
        log_section "Pod: $pod"

        # Check for errors
        local errors=$(kubectl logs "$pod" -n "$namespace" --tail="$tail_lines" 2>/dev/null | \
            grep -iE "error|exception|fatal|failed" || true)

        if [ -n "$errors" ]; then
            error_count=$((error_count + 1))
            log_error "Errors found in pod logs:"
            echo "$errors" | tail -10
            echo
        fi

        # Check for warnings
        local warnings=$(kubectl logs "$pod" -n "$namespace" --tail="$tail_lines" 2>/dev/null | \
            grep -iE "warn|warning" || true)

        if [ -n "$warnings" ]; then
            warn_count=$((warn_count + 1))
            log_error "Warnings found in pod logs:"
            echo "$warnings" | tail -5
            echo
        fi
    done

    # Summary
    echo
    if [ $error_count -eq 0 ] && [ $warn_count -eq 0 ]; then
        log_success "No errors or warnings found in logs"
    else
        [ $error_count -gt 0 ] && log_error "Found errors in $error_count pod(s)"
        [ $warn_count -gt 0 ] && log_error "Found warnings in $warn_count pod(s)"
    fi

    return 0
}

check_health_probe() {
    local service=$1
    local namespace=$2
    local port=${3:-8080}
    local path=${4:-/actuator/health}

    log_info "Checking health probe: $service:$port$path"

    # Port-forward in background
    local local_port=$((8000 + RANDOM % 1000))
    kubectl port-forward -n "$namespace" "svc/$service" "$local_port:$port" &>/dev/null &
    local pf_pid=$!

    # Wait for port-forward to establish
    sleep 2

    # Call health endpoint
    local response=$(curl -s -w "\n%{http_code}" "http://localhost:$local_port$path" 2>/dev/null || echo "FAILED")
    local http_code=$(echo "$response" | tail -1)
    local body=$(echo "$response" | head -n -1)

    # Kill port-forward
    kill $pf_pid 2>/dev/null || true

    # Evaluate response
    if [ "$http_code" = "200" ]; then
        log_success "Health probe OK (HTTP $http_code)"
        echo "$body" | jq '.' 2>/dev/null || echo "$body"
        return 0
    else
        log_error "Health probe FAILED (HTTP $http_code)"
        echo "$body"
        return 1
    fi
}

# Check all actuator endpoints
# Usage: check_actuator_endpoints <service-name> <namespace> <port>
check_actuator_endpoints() {
    local service=$1
    local namespace=$2
    local port=${3:-8080}

    log_section "Actuator Endpoints: $service"

    local endpoints=("health" "info" "metrics")

    for endpoint in "${endpoints[@]}"; do
        echo
        log_info "Endpoint: /actuator/$endpoint"
        check_health_probe "$service" "$namespace" "$port" "/actuator/$endpoint"
    done
}


# ===================== ETL Initializer =====================
cd "${PROJECT_ROOT}/services/etl_initializer" || exit_error "Missing directory: ${PROJECT_ROOT}/services/etl_initializer"

SERVICE_NAME="etl-initializer"

log_section "Installing ETL Initializer Service"

log_info "Running Maven build..."

if ! mvn clean package -Dmaven.test.skip=true; then
    log_error "Maven build failed"
    exit 1
fi

log_success "Maven build completed"

log_info "Building Docker image with tag: etl-initializer:0.0.1-SNAPSHOT"

if ! docker build -t etl-initializer:0.0.1-SNAPSHOT .; then
    log_error "Docker image build failed"
    exit 1
else
	if docker images etl-initializer:0.0.1-SNAPSHOT --format "{{.Repository}}:{{.Tag}}" | grep -q "etl-initializer:0.0.1-SNAPSHOT"; then
	    log_success "Image verified in local Docker registry"
	else
	    log_error "Image not found in local Docker registry!"
	    exit 1
	fi
	log_success "Build completed: etl-initializer:0.0.1-SNAPSHOT"
fi

if ! kubectl apply -f "./k8s_manifist/etl-initializer-deployment.yaml" -n "${A_NAMESPACE}"; then
	log_error "Deployment manifest failed"
	exit 1
else
	log_success "Deployment manifest applied"
fi

# Monitor health
monitor_pod_health "$SERVICE_NAME" "$A_NAMESPACE" 120

# Check pod status
POD_STATUS=$(get_pod_status "$SERVICE_NAME" "$A_NAMESPACE")
log_info "Pod Status: $POD_STATUS"

# Check logs for errors
scan_pod_logs "$SERVICE_NAME" "$A_NAMESPACE" 60

log_success "ETL Initializer service installed successfully"

# ===================== Load ETL Configuration YAML =====================
log_section "Loading ETL Configuration"

ETL_YAML_FILE="${PROJECT_ROOT}/services/testData/etl-data-v1.yaml"

if [ ! -f "$ETL_YAML_FILE" ]; then
    log_error "ETL YAML file not found: $ETL_YAML_FILE"
    log_error "Please ensure backup/etl-data-v1.yaml exists before running installer"
    exit 1
fi

log_info "Waiting for etl-initializer pod to be ready..."
if ! kubectl wait --for=condition=ready pod -l app=etl-initializer -n "${A_NAMESPACE}" --timeout=120s; then
    log_error "etl-initializer pod did not become ready in time"
    exit 1
fi

ETL_POD=$(kubectl get pod -n "${A_NAMESPACE}" -l app=etl-initializer -o jsonpath='{.items[0].metadata.name}')
log_info "ETL Initializer pod: $ETL_POD"

log_info "Copying ETL configuration YAML to pod..."
if ! kubectl cp "$ETL_YAML_FILE" "${A_NAMESPACE}/${ETL_POD}:/data/uploads/etl-data-v1.yaml"; then
    log_error "Failed to copy YAML file to pod"
    exit 1
fi

log_success "ETL configuration file uploaded successfully"
log_info "Monitoring ETL configuration processing (this may take up to 60 seconds)..."

# Wait for processing
sleep 15

# Check processing logs
kubectl logs -n "${A_NAMESPACE}" "$ETL_POD" --tail=50 | grep -E "Processing file|Successfully processed|version" || true

log_info "Verifying loaders were created..."
kubectl exec -n monitoring-infra postgres-postgresql-0 -- \
    env PGPASSWORD=HaAirK101348App psql -U alerts_user -d alerts_db -c \
    "SELECT loader_code, load_status, min_interval_seconds FROM loader.loader;" || \
    log_warn "Could not verify loader creation (this is normal on first run)"

log_success "ETL configuration loaded successfully"

read -r -p "Proceed? (Y/n): " ans < /dev/tty
[[ "${ans:-Y}" =~ ^[Yy]$ ]] || exit 1

# ===================== DATA Generator =====================
cd "${PROJECT_ROOT}/services/dataGenerator" || exit_error "Missing directory: ${PROJECT_ROOT}/services/dataGenerator"

SERVICE_NAME="data-generator"

log_section "Installing data-generator Service"

log_info "Running Maven build..."

if ! mvn clean package -Dmaven.test.skip=true; then
    log_error "Maven build failed"
    exit 1
fi

log_success "Maven build completed"


log_info "Building Docker image with tag: data-generator:0.0.1-SNAPSHOT"


if ! docker build -t data-generator:0.0.1-SNAPSHOT .; then
    log_error "Docker image build failed"
    exit 1
else
	if docker images data-generator:0.0.1-SNAPSHOT --format "{{.Repository}}:{{.Tag}}" | grep -q "data-generator:0.0.1-SNAPSHOT"; then
	    log_success "Image verified in local Docker registry"
	else
	    log_error "Image not found in local Docker registry!"
	    exit 1
	fi
	log_success "Build completed: data-generator:0.0.1-SNAPSHOT"
fi

if ! kubectl apply -f "./k8s_manifist/data-generator-deployment.yaml" -n $A_NAMESPACE; then
	log_error "Deployment manifest failed"
	exit 1
else
	log_success "Deployment manifest applied"
fi

# Monitor health

monitor_pod_health "$SERVICE_NAME" "$A_NAMESPACE" 60



# check pod status
POD_STATUS=$(get_pod_status "$SERVICE_NAME" "$A_NAMESPACE")
log_info "Pod Status: $POD_STATUS" 
# Check logs for errors
scan_pod_logs "$SERVICE_NAME" "$A_NAMESPACE" 60

# Test health probes
check_actuator_endpoints "$SERVICE_NAME" "$A_NAMESPACE" 8080

log_success "data-generator service installed successfully"

read -r -p "Proceed? (Y/n): " ans < /dev/tty
[[ "${ans:-Y}" =~ ^[Yy]$ ]] || exit 1

# ===================== signal loader =====================
cd "${PROJECT_ROOT}/services/loader" || exit_error "Missing directory: ${PROJECT_ROOT}/services/loader"

SERVICE_NAME="signal-loader"

log_section "Installing signal-loader Service"

log_info "Running Maven build..."

if ! mvn clean package -Dmaven.test.skip=true; then
    log_error "Maven build failed"
    exit 1
fi

log_success "Maven build completed"


log_info "Building Docker image with tag: signal-loader:0.0.1-SNAPSHOT"


if ! docker build -t signal-loader:0.0.1-SNAPSHOT .; then
    log_error "Docker image build failed"
    exit 1
else
	if docker images signal-loader:0.0.1-SNAPSHOT --format "{{.Repository}}:{{.Tag}}" | grep -q "signal-loader:0.0.1-SNAPSHOT"; then
	    log_success "Image verified in local Docker registry"
	else
	    log_error "Image not found in local Docker registry!"
	    exit 1
	fi
	log_success "Build completed: signal-loader:0.0.1-SNAPSHOT"
fi

if ! kubectl apply -f "./k8s_manifist/loader-deployment.yaml" -n "${A_NAMESPACE}"; then
	log_error "Deployment manifest failed"
	exit 1
else
	log_success "Deployment manifest applied"
fi


# check pos sttaus

monitor_pod_health "$SERVICE_NAME" "$A_NAMESPACE" 120


# check pod status
POD_STATUS=$(get_pod_status "$SERVICE_NAME" "$A_NAMESPACE")
log_info "Pod Status: $POD_STATUS"
# Check logs for errors
scan_pod_logs "$SERVICE_NAME" "$A_NAMESPACE" 60

# Test health probes
check_actuator_endpoints "$SERVICE_NAME" "$A_NAMESPACE" 8080

log_success "loader-deployment service installed successfully"




